# Word Embedding Analogies ðŸ§ ðŸ“š

This project explores how word embeddings capture semantic relationships between words. Using pre-trained GloVe vectors, it demonstrates how vector arithmetic can solve analogy problems like:

> "**man** is to **king** as **woman** is to **queen**"

The project is inspired by DeepLearning.AI's Deep Learning Specialization (Course 5: Sequence Models).

## ðŸ“Œ Project Overview

The notebook performs the following tasks:

- Loads 50-dimensional GloVe word vectors
- Computes cosine similarity between word pairs
- Solves word analogies using vector arithmetic
- Demonstrates how embeddings capture semantic meaning
